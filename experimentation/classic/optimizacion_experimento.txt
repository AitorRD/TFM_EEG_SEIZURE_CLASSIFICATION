===============================================================================
OPTIMIZACIÓN DE EXPERIMENTO - EEG SEIZURE CLASSIFICATION
Hardware: 48GB RAM + NVIDIA GeForce RTX 2060
Fecha: 11 de Febrero de 2026
===============================================================================

==============================================================================
CONFIGURACIÓN GENERAL
==============================================================================
- Nombre experimento: "eeg_seizure_experiment_full"
- Tipo: ML + DL combinados
- Random state: 42
- N_jobs: -1 (usar todos los cores CPU)
- Device: CUDA (RTX 2060)

==============================================================================
OPTIMIZACIÓN DE OPTUNA (HYPERPARAMETER TUNING)
==============================================================================
ANTES:
  - n_trials: 100
  - n_startup_trials: 10
  - n_warmup_steps: 5
  - save_visualizations: false

DESPUÉS:
  ✓ n_trials: 150 (↑50%)
  ✓ n_startup_trials: 15 (↑50%)
  ✓ n_warmup_steps: 8 (↑60%)
  ✓ save_visualizations: true (activado para análisis)

JUSTIFICACIÓN: Con 48GB RAM y RTX 2060, podemos permitirnos más trials
para encontrar mejores hiperparámetros sin problemas de memoria.

==============================================================================
MODELOS DE MACHINE LEARNING TRADICIONAL
==============================================================================
TODOS LOS MODELOS ACTIVADOS CON TUNING OPTUNA:

✓ Logistic Regression (LR)
  - enabled: true
  - C: 0.001-100 (log)
  - Solver: lbfgs, liblinear, saga
  - Max iter: 1000-3000

✓ Random Forest (RF)
  - enabled: true
  - n_estimators: 50-300
  - max_depth: 10-50
  - min_samples_split: 2-10
  - min_samples_leaf: 1-4

✓ Support Vector Classifier (SVC)
  - enabled: true
  - C: 0.1-100 (log)
  - Kernel: linear, rbf, poly
  - Gamma: scale, auto

✓ K-Nearest Neighbors (KNN)
  - enabled: true (NUEVO - antes desactivado)
  - n_neighbors: 3-15
  - Weights: uniform, distance
  - Metric: euclidean, manhattan, minkowski

✓ XGBoost (XGB)
  - enabled: true
  - n_estimators: 50-300
  - max_depth: 3-10
  - learning_rate: 0.001-0.3 (log)
  - subsample: 0.6-1.0
  - colsample_bytree: 0.6-1.0

==============================================================================
CONFIGURACIÓN DEEP LEARNING
==============================================================================
PARÁMETROS GENERALES:

ANTES:
  - enabled: false
  - epochs: 50
  - batch_size: 32
  - early_stopping patience: 10
  - early_stopping min_delta: 0.001

DESPUÉS:
  ✓ enabled: true
  ✓ epochs: 80 (↑60%)
  ✓ batch_size: 128 (↑300% - aprovecha RTX 2060 + 48GB RAM)
  ✓ early_stopping patience: 15 (↑50%)
  ✓ early_stopping min_delta: 0.0005 (más fino)

JUSTIFICACIÓN: 
- Batch size 128 aprovecha memoria GPU (6GB en RTX 2060)
- 48GB RAM permiten cargar datasets grandes sin problemas
- Mayor patience para permitir convergencia completa

==============================================================================
MODELOS DEEP LEARNING ACTIVADOS
==============================================================================

1. TRANSFORMER
   ✗ enabled: false (IGNORADO por petición del usuario)

2. CNN 1D (Convolutional Neural Network) ⭐
   ✓ enabled: true

   DEFAULT PARAMS - ANTES:
     - conv1_channels: 64
     - conv2_channels: 128
     - fc_hidden: 64
     - dropout: 0.5

   DEFAULT PARAMS - DESPUÉS:
     ✓ conv1_channels: 128 (↑100%)
     ✓ conv2_channels: 256 (↑100%)
     ✓ fc_hidden: 128 (↑100%)
     ✓ dropout: 0.4 (optimizado)

   OPTUNA SEARCH SPACE - ANTES:
     - conv1: [32, 64, 128]
     - conv2: [64, 128, 256]
     - fc_hidden: [32, 64, 128]
     - dropout: 0.2-0.7

   OPTUNA SEARCH SPACE - DESPUÉS:
     ✓ conv1: [64, 128, 256] (↑rango)
     ✓ conv2: [128, 256, 512] (↑rango)
     ✓ fc_hidden: [64, 128, 256] (↑rango)
     ✓ dropout: 0.2-0.6 (ajustado)

3. LSTM RNN (Long Short-Term Memory) ⭐
   ✓ enabled: true

   DEFAULT PARAMS - ANTES:
     - hidden_dim: 128
     - num_layers: 2
     - dropout: 0.3
     - bidirectional: true

   DEFAULT PARAMS - DESPUÉS:
     ✓ hidden_dim: 256 (↑100%)
     ✓ num_layers: 3 (↑50%)
     ✓ dropout: 0.3 (mantener)
     ✓ bidirectional: true (mantener)

   OPTUNA SEARCH SPACE - ANTES:
     - hidden_dim: [64, 128, 256]
     - num_layers: 1-3
     - dropout: 0.1-0.5

   OPTUNA SEARCH SPACE - DESPUÉS:
     ✓ hidden_dim: [128, 256, 512] (↑rango)
     ✓ num_layers: 2-4 (↑mínimo y máximo)
     ✓ dropout: 0.1-0.5 (mantener)

4. GRU RNN (Gated Recurrent Unit) ⭐
   ✓ enabled: true

   DEFAULT PARAMS - ANTES:
     - hidden_dim: 128
     - num_layers: 2
     - dropout: 0.3
     - bidirectional: true

   DEFAULT PARAMS - DESPUÉS:
     ✓ hidden_dim: 256 (↑100%)
     ✓ num_layers: 3 (↑50%)
     ✓ dropout: 0.3 (mantener)
     ✓ bidirectional: true (mantener)

   OPTUNA SEARCH SPACE - ANTES:
     - hidden_dim: [64, 128, 256]
     - num_layers: 1-3
     - dropout: 0.1-0.5

   OPTUNA SEARCH SPACE - DESPUÉS:
     ✓ hidden_dim: [128, 256, 512] (↑rango)
     ✓ num_layers: 2-4 (↑mínimo y máximo)
     ✓ dropout: 0.1-0.5 (mantener)

==============================================================================
APROVECHAMIENTO DE RECURSOS
==============================================================================

NVIDIA GeForce RTX 2060 (6GB VRAM):
  ✓ Batch size aumentado a 128 (usa ~3-4GB VRAM)
  ✓ Modelos más profundos (hasta 512 hidden dims en RNNs)
  ✓ Canales convolucionales hasta 512 en CNN
  ✓ Training con CUDA activado

48GB RAM:
  ✓ Carga completa de datasets sin paginación
  ✓ Feature extraction en memoria
  ✓ Cross-validation sin problemas de memoria
  ✓ Múltiples trials de Optuna en paralelo
  ✓ XAI (SHAP/LIME) con muestras grandes

CPU (todos los cores):
  ✓ n_jobs = -1 en modelos ML
  ✓ Parallel processing en feature extraction
  ✓ Cross-validation paralela

==============================================================================
CONFIGURACIONES MANTENIDAS
==============================================================================
- Feature extraction: ACTIVADO (tsfresh)
- Feature selection: ACTIVADO (SelectKBest, k=50)
- Cross-validation: ACTIVADO (5 folds, shuffle=true)
- XAI: ACTIVADO (SHAP + LIME para ML)
- Learning rate scheduler: ACTIVADO (ReduceLROnPlateau)
- Class balancing: ACTIVADO (balanced weights)

==============================================================================
EXPERIMENTOS QUE SE EJECUTARÁN
==============================================================================

FASE 1: MACHINE LEARNING TRADICIONAL
  1. Logistic Regression + Optuna (150 trials)
  2. Random Forest + Optuna (150 trials)
  3. Support Vector Classifier + Optuna (150 trials)
  4. K-Nearest Neighbors + Optuna (150 trials)
  5. XGBoost + Optuna (150 trials)
  
  Para cada modelo:
    - Cross-validation 5-fold
    - Feature selection optimizada
    - SHAP explanations
    - LIME explanations
    - Métricas completas en test set

FASE 2: DEEP LEARNING
  1. CNN 1D + Optuna (150 trials)
     - Arquitectura optimizada para features
     - Batch size: 128
     - 80 épocas máximo con early stopping
     
  2. LSTM RNN + Optuna (150 trials)
     - Bidirectional LSTM
     - Hidden dims: 256-512
     - 3-4 capas
     - Batch size: 128
     
  3. GRU RNN + Optuna (150 trials)
     - Bidirectional GRU
     - Hidden dims: 256-512
     - 3-4 capas
     - Batch size: 128

==============================================================================
MÉTRICAS A REPORTAR
==============================================================================
- Accuracy
- Precision (weighted)
- Recall (weighted)
- F1-score (weighted)
- F1-macro
- F1-micro
- ROC-AUC

==============================================================================
ESTIMACIÓN DE TIEMPO DE EJECUCIÓN
==============================================================================
Con la configuración optimizada:

Machine Learning:
  - Por modelo: ~30-60 min (con 150 trials Optuna)
  - Total 5 modelos ML: ~2.5-5 horas

Deep Learning:
  - CNN: ~1-2 horas (más rápido que RNNs)
  - LSTM: ~3-5 horas (más complejo)
  - GRU: ~2.5-4 horas (similar a LSTM)
  - Total 3 modelos DL: ~6.5-11 horas

TIEMPO TOTAL ESTIMADO: 9-16 horas
(depende de convergencia, early stopping, y complejidad del dataset)

==============================================================================
ARCHIVOS DE SALIDA ESPERADOS
==============================================================================
- images/results/ml_test_metrics.png (comparativas ML)
- images/xai/shap_*.png (explicaciones SHAP por modelo)
- images/xai/lime_*.png (explicaciones LIME por modelo)
- data/processed/optuna_results/*.csv (resultados de tuning)
- data/processed/optuna_results/*.html (visualizaciones interactivas)
- data/processed/selected_features.csv (features seleccionadas)
- models/ (modelos entrenados guardados)

==============================================================================
COMANDO PARA LANZAR EXPERIMENTO
==============================================================================
cd experimentation/classic
python ml_experiments.py

NOTA: El script detectará automáticamente:
  - GPU disponible (CUDA)
  - Número de cores CPU
  - Memoria RAM disponible
  - Y ajustará workers/procesos en consecuencia

==============================================================================
RECOMENDACIONES DURANTE LA EJECUCIÓN
==============================================================================
1. Monitorear uso de GPU: nvidia-smi
2. Monitorear uso de RAM: Task Manager
3. No cerrar la terminal durante la ejecución
4. Los resultados intermedios se guardan automáticamente
5. Si se interrumpe, Optuna puede retomar desde último trial
6. Logs detallados se mostrarán en consola

==============================================================================
FIN DEL DOCUMENTO DE OPTIMIZACIÓN
==============================================================================
